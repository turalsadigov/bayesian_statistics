---
title: Bayes Rules! - Chapter 6 and Chapter 7
author: Your Name
date: today
format: 
    html:
      theme: 
        light: united
        dark: darkly
editor: visual
chunk_output_type: console
fig-align: center
always_allow_html: true
toc: true
toc-location: right
number-sections: false
page-layout: article
code-overflow: scroll
code-line-numbers: false
code-copy: true
execute:
  echo: true
  warning: false
  eval: true
  output: true
  error: false
  freeze: true
  out.width: "100%"
  cache: true
title-block-banner: true
bibliography: references.bib
---

In addition to usual packages, `tidyverse` (@tidyverse), `stats2data` (@stats2data), `janitor` (@janitor), `bayesrules` (@bayesrules) and `bayesplot` (@bayesplot), we also use `rstan` @rstan in this chapter.

## Libraries

```{r}
library(tidyverse)
library(stats2data)
library(rstan)
library(janitor)
library(bayesplot)
library(bayesrules)
```

# Chapter 6

## Grid Approximation

Take beta prior

$$
\pi \sim Beta(2,2)
$$

and binomial data model

$$
Y|\pi \sim Binomial(10, \pi)
$$

Observe evidence/data of 9 successes out of 10 trials. Then we know the posterior will

$$
\pi \sim Beta(2+9,2+1) = Beta(11, 3)
$$

Now we approximate this posterior using the following steps.

1.  Define a discrete grid of possible values.

2.  Evaluate the prior pdf and likelihood function at each grid value.

3.  Obtain a discrete approximation of the posterior pdf by:

    -   calculating the product of prior and likelihood at each grid value;

    -   *normalizing* the products so that they sum to 1 across all grid values

4.  Randomly sample grid values with respect to their corresponding normalized posterior probabilities.

First we do this for only few possible points:

$$
\pi \in \{0, 0.2, 0.4, 0.8, 1 \}
$$

```{r}
grid_data <- 
  tibble(pi_grid = seq(from = 0, to = 1, length = 6)) %>% #step1
  mutate(prior = dbeta(pi_grid, 2, 2),
         likelihood = dbinom(9, 10, pi_grid)) %>% #step2
   mutate(unnormalized = likelihood * prior,
         posterior = unnormalized / sum(unnormalized)) #step3
grid_data %>% 
  round(2)

grid_data %>% 
  ggplot(aes(x = pi_grid)) + 
  geom_point(aes(y = prior/sum(prior), size = 2), color = 'red') + 
  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = prior/sum(prior))) +
  geom_point(aes(y = posterior, size = 2), color='darkgreen') + 
  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior)) 


# step4: sample from the discretized posterior
set.seed(84735)
post_sample <- 
  grid_data %>% 
  select(pi_grid, posterior) %>% 
  sample_n(size = 10000, 
           weight = posterior, 
           replace = TRUE) %>% 
  select(pi_grid)

post_sample

post_sample %>% 
  count(pi_grid) %>% 
  mutate(perc = n/sum(n))

grid_data %>% 
  round(2)

# Histogram of the grid simulation with posterior pdf
post_sample %>% 
  ggplot(aes(x = pi_grid)) + 
  geom_histogram(aes(y = ..density..), color = "white", fill = "darkgreen") + 
  geom_density(linewidth = 1, color = 'red') +
  stat_function(fun = dbeta, args = list(11, 3), color = 'blue', linewidth = 1) + 
  lims(x = c(0, 1))
```

Lets do it again for 100 values.

```{r}
grid_data <- 
  tibble(pi_grid = seq(from = 0, to = 1, length = 100)) %>% #step1
  mutate(prior = dbeta(pi_grid, 2, 2),
         likelihood = dbinom(9, 10, pi_grid)) %>% #step2
   mutate(unnormalized = likelihood * prior,
         posterior = unnormalized / sum(unnormalized)) #step3
grid_data %>% 
  round(2)

grid_data %>% 
  ggplot(aes(x = pi_grid)) + 
  geom_point(aes(y = prior/sum(prior)), color = 'red') + 
  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = prior/sum(prior))) +
  geom_point(aes(y = posterior), color='darkgreen') + 
  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior)) 


# step4: sample from the discretized posterior
set.seed(84735)
post_sample <- 
  grid_data %>% 
  select(pi_grid, posterior) %>% 
  sample_n(size = 10000, 
           weight = posterior, 
           replace = TRUE) %>% 
  select(pi_grid)


post_sample %>% 
  ggplot(aes(pi_grid)) +
  geom_density()

# Histogram of the grid simulation with posterior pdf
post_sample %>% 
  ggplot(aes(x = pi_grid)) + 
  geom_histogram(aes(y = ..density..), color = "white", fill = "darkgreen") + 
  geom_density(linewidth = 1, color = 'red') +
  stat_function(fun = dbeta, args = list(11, 3), color = 'blue', linewidth = 1) + 
  lims(x = c(0, 1))
```

## Gamma-Poisson example

Take Gamma prior

$$
\lambda \sim Gamma(3,1)
$$

and Poisson data model

$$
Y|\lambda \sim Poisson(\lambda)
$$

Observe evidence/data of $Y_1 = 2, Y_2 = 8$. Then we know the posterior will

$$
\lambda | \{Y_1, Y_2\} \sim Gamma(3 + Y_1 + Y_2, 1 + 2) = Gamma(13, 3)
$$

```{r}
grid_data <- 
  tibble(lambda_grid = seq(from = 0, to = 15, length = 100)) %>% #step1
  mutate(prior = dgamma(lambda_grid, 3, 1),
         likelihood = dpois(2, lambda_grid)*dpois(8, lambda_grid)) %>% #step2
   mutate(unnormalized = likelihood * prior,
         posterior = unnormalized / sum(unnormalized)) #step3
grid_data %>% 
  round(2)

grid_data %>% 
  ggplot(aes(x = lambda_grid)) + 
  geom_point(aes(y = prior/sum(prior)), color = 'red') + 
  geom_segment(aes(x = lambda_grid, xend = lambda_grid, y = 0, yend = prior/sum(prior))) +
  geom_point(aes(y = posterior), color='darkgreen') + 
  geom_segment(aes(x = lambda_grid, xend = lambda_grid, y = 0, yend = posterior)) +
  ylab('') +
  xlab('lambda')


# step4: sample from the discretized posterior
set.seed(84735)
post_sample <- 
  grid_data %>% 
  select(lambda_grid, posterior) %>% 
  sample_n(size = 10000, 
           weight = posterior, 
           replace = TRUE) %>% 
  select(lambda_grid)


post_sample %>% 
  ggplot(aes(lambda_grid)) +
  geom_density()

# Histogram of the grid simulation with posterior pdf
post_sample %>% 
  ggplot(aes(x = lambda_grid)) + 
  geom_histogram(aes(y = ..density..), color = "white", fill = "darkgreen") + 
  geom_density(linewidth = 1, color = 'red') +
  stat_function(fun = dgamma, args = list(13, 3), color = 'blue', linewidth = 1) + 
  lims(x = c(0, 15))
```

## Markov Chain Monte Carlo (MCMC)

```{r}
# STEP 1: DEFINE the model
bb_model <- "
  data {
    int<lower = 0, upper = 10> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(10, pi);
    pi ~ beta(2, 2);
  }
"

# STEP 2: SIMULATE the posterior
bb_sim <- stan(model_code = bb_model, 
               data = list(Y = 9), 
               chains = 4, 
               iter = 5000*2, 
               seed = 84735)
```

Extract \pi values from `rstanfit` object.

```{r}
as.array(bb_sim, pars = "pi") %>% 
  head(4)
```

Trace plot (chain tracing through sample space of the posterior distribution)

```{r}
mcmc_trace(bb_sim, 
           pars = "pi",
           size = 0.1)
mcmc_trace(bb_sim)
mcmc_trace(bb_sim, pars = "pi")
```

Density

```{r}
# Histogram of the Markov chain values
mcmc_hist(bb_sim, pars = "pi") + 
  yaxis_text(TRUE) + 
  ylab("count")

# Density plot of the Markov chain values
mcmc_dens(bb_sim, pars = "pi") + 
  yaxis_text(TRUE) + 
  ylab("density")
```

Gamma-Poisson example

Assume

$$
Y \sim Poisson(\lambda)
$$

where

$$
\lambda \sim Gamma(3, 1).
$$

```{r}
# STEP 1: DEFINE the model
gp_model <- "
  data {
    int<lower = 0> Y[2];
  }
  parameters {
    real<lower = 0> lambda;
  }
  model {
    Y ~ poisson(lambda);
    lambda ~ gamma(3, 1);
  }
"

# STEP 2: SIMULATE the posterior
gp_sim <- stan(model_code = gp_model, 
               data = list(Y = c(2,8)), 
               chains = 4, 
               iter = 5000*2, 
               seed = 84735)
```

Trace.

```{r}
# Trace plots of the 4 Markov chains
mcmc_trace(gp_sim, 
           pars = "lambda", 
           size = 0.1)

# Histogram of the Markov chain values
mcmc_hist(gp_sim, pars = "lambda") + 
  yaxis_text(TRUE) + 
  ylab("count")

# Density plot of the Markov chain values
mcmc_dens(gp_sim, pars = "lambda") + 
  yaxis_text(TRUE) + 
  ylab("density")
```

## Example of instability in short runs

```{r}
# STEP 2: SIMULATE the posterior
bb_sim_short <- stan(model_code = bb_model, 
                     data = list(Y = 9), 
                     chains = 4,
                     iter = 50*2, 
                     seed = 84735)
# Trace plots of short chains
mcmc_trace(bb_sim_short, pars = "pi")

# Density plots of individual short chains
mcmc_dens_overlay(bb_sim_short, pars = "pi")
```

## Effective sample size

```{r}
neff_ratio(bb_sim, pars = c("pi"))
neff_ratio(bb_sim, pars = c("pi"))*20000
```

## Autocorrelation

```{r}
mcmc_trace(bb_sim, pars = "pi")
mcmc_acf(bb_sim, pars = "pi")
```

## Thinning the time series

```{r}
# Simulate a thinned MCMC sample
thinned_sim <- stan(model_code = bb_model, 
                    data = list(Y = 9), 
                    chains = 4, 
                    iter = 5000*2, 
                    seed = 84735, 
                    thin = 10)

# Check out the results
mcmc_trace(thinned_sim, pars = "pi")
mcmc_acf(thinned_sim, pars = "pi")
```

## rstan and rstanarm

-   Hamiltonian Monte Carlo Algorithm
-   Advises against thinning

## R-hat

```{r}
rhat(bb_sim, pars = "pi")
```

# Chapter 7

Assume the following Normal-Normal model:

$$
Y|\mu \sim Normal(\mu, 0.75^2)
$$

$$
\mu \sim Normal(0, 1^2)
$$

We observe data:

$$
Y = 6.25
$$

Then our posterior is:

$$
\mu| Y = 6.25 \sim Normal(4, 0.6^2)
$$

### FIND THIS POSTERIOR THROUGH SIMULATION.

Just independent Monte Carlo simulation

```{r}
set.seed(84375)
mc_tour <- tibble(mu = rnorm(5000, 
                             mean = 4, 
                             sd = 0.6))
mc_tour %>% 
  ggplot(aes(x = mu)) + 
  geom_histogram(aes(y = ..density..), color = "white", bins = 50) + 
  stat_function(fun = dnorm, args = list(4, 0.6), color = "blue")
```

Metropolis

```{r}
current <- 3
set.seed(8)
proposal <- runif(1, 
                  min = current - 1, 
                  max = current + 1)
proposal

# prior * likelihood
proposal_plaus <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)
proposal_plaus

current_plaus  <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)
current_plaus

proposal_plaus / current_plaus

alpha <- min(1, proposal_plaus / current_plaus)
alpha

next_stop <- sample(c(proposal, current),
                    size = 1, 
                    prob = c(alpha, 1-alpha))
next_stop
```

Automate

```{r}
one_mh_iteration <- function(w, current){
 # STEP 1: Propose the next chain location
 proposal <- runif(1, min = current - w, max = current + w)
  
 # STEP 2: Decide whether or not to go there
 proposal_plaus <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)
 current_plaus  <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)
 alpha <- min(1, proposal_plaus / current_plaus)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 # Return the results
 return(data.frame(proposal, alpha, next_stop))
}
```

```{r}
set.seed(8)
one_mh_iteration(w = 1, current = 3)
set.seed(83)
one_mh_iteration(w = 1, current = 3)
set.seed(7)
one_mh_iteration(w = 1, current = 3)
```

Next time: 7.3-7.8
